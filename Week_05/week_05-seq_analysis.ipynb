{"cells":[{"cell_type":"markdown","id":"j5zATIOM_YDe","metadata":{"id":"j5zATIOM_YDe"},"source":["## Preliminary operations\n","\n","We will use [PyTorch](https://pytorch.org/) for our experiments. This package is already installed if you are using [Google colab](https://colab.research.google.com), a notebook-like system for running jupyter notebooks on virtual machines provisioned for free on the cloud. This has several advantages, and specifically allows us to use a GPU, though with some time limitations, and it requires an active Google account. In the rest of the notebook, it is expected that running is done on colab, although everything should execute smoothly also in other environments, provided that all packages have been installed (you can download a `requirements.txt.` file from the course Web site in order to automatically perform this installation on a local machine via `pip`). However, be aware that running this code on a machine not equipped with a GPU is likely to require a huge time in order to complete.\n","\n","Let's check that the virtual machine we are using comes with a GPU: if you have opened this notebook in colab, look in the upper-right part of the browser content, right below your google avatar: if you see this"]},{"cell_type":"markdown","id":"Iup8QPXNAnmQ","metadata":{"id":"Iup8QPXNAnmQ"},"source":["![image.png](data:image/png;base64,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)"]},{"cell_type":"markdown","id":"2d8814krBtDb","metadata":{"id":"2d8814krBtDb"},"source":["you are good to go: your machine has a GPU. In all other cases, use the *Runtime/Change runtime type* menu item and select any available runtime featuring a GPU. If none is made available to you, this means that you have exceeded the time limits allowed for the free use of colab. Otherwise, you should now see the above shown graphics in the upper-right part of your screen.\n","\n","As a first operation, let's import the pyTorch library and use it to double-check the GPU availability."]},{"cell_type":"code","execution_count":null,"id":"vS5qIZAA-aet","metadata":{"id":"vS5qIZAA-aet"},"outputs":[],"source":["import torch\n","\n","print(f'pytorch version: {torch.__version__}')\n","\n","num_gpu = torch.cuda.device_count()\n","print(f'{num_gpu} GPU available')"]},{"cell_type":"markdown","id":"-WFYLUKZTDK3","metadata":{"id":"-WFYLUKZTDK3"},"source":["Indeed, we have a GPU. Note that the pytorch version is made up of two elements, separated by a «plus» symbol: on the left, you find the version of the software, while on the right there is the CUDA version (CUDA is the name of the API framework used by the GPU). If you want to find out some more detail about the GPU, you can run the following cell (note that the exclamation mark implies that the cell is executed as a bash command)."]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"Mlu1-zaE_ERs"},"id":"Mlu1-zaE_ERs","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"YICNaHevrt4z","metadata":{"id":"YICNaHevrt4z"},"source":["### Next"]},{"cell_type":"markdown","id":"tOjR3WWbWMA0","metadata":{"id":"tOjR3WWbWMA0"},"source":["We'll need to install some packages which are not automatically provided in colab."]},{"cell_type":"code","execution_count":null,"id":"90lYXQbZVvGr","metadata":{"id":"90lYXQbZVvGr"},"outputs":[],"source":["!pip install -q datasets umap-learn transformers[torch] sacrebleu nltk rouge_score"]},{"cell_type":"markdown","id":"fenY3tWEpA2B","metadata":{"id":"fenY3tWEpA2B"},"source":["The first part of this practical part involves the generation of text using RNNs, thus we need some text to train the generator: we will use a corpus of Shakespeare's works."]},{"cell_type":"code","execution_count":null,"id":"PfC8AaHrgPDt","metadata":{"id":"PfC8AaHrgPDt"},"outputs":[],"source":["!wget https://raw.githubusercontent.com/josehoras/LSTM-Frameworks/master/shakespeare.txt"]},{"cell_type":"markdown","id":"rET9DlsmrB1M","metadata":{"id":"rET9DlsmrB1M"},"source":["We also need to import several packages."]},{"cell_type":"code","execution_count":null,"id":"FvxW7dzxQrzJ","metadata":{"id":"FvxW7dzxQrzJ"},"outputs":[],"source":["import gc\n","import time\n","\n","from datasets import load_dataset\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import Adam\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data import WeightedRandomSampler\n","\n","from transformers import AutoTokenizer, AutoModel\n","from transformers import AutoModelForSequenceClassification\n","from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n","from transformers import Trainer, TrainingArguments\n","\n","from tqdm import tqdm\n","\n","from umap import UMAP"]},{"cell_type":"markdown","id":"cNnE7XK9sjrU","metadata":{"id":"cNnE7XK9sjrU"},"source":["The following cell defines a `CustomDataset` class, which we will use for creating labeled samples $(x, t)$, where $x$ and $t$ represent two adjacent chars in the corpus."]},{"cell_type":"code","execution_count":null,"id":"SWRjwtbsso-D","metadata":{"id":"SWRjwtbsso-D"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, data_name):\n","        self.data = open(data_name + '.txt', 'r').read()\n","        chars = sorted(set(self.data))\n","        self.vocab_size = len(chars)\n","        self.char_to_idx = {ch: i for i, ch in enumerate(chars)}\n","        self.idx_to_char = {i: ch for i, ch in enumerate(chars)}\n","        print(f'data has {len(self.data)} characters,'\n","              f' {self.vocab_size} unique.')\n","\n","    def __getitem__(self, index):\n","        #current char\n","        x = self.char_to_idx[self.data[index]]\n","        x = torch.tensor([x])\n","        x = F.one_hot(x, num_classes=self.vocab_size)\n","        x = x.type(torch.FloatTensor)\n","\n","        # next char\n","        t = self.char_to_idx[self.data[index + (index < (self.__len__() - 1))]]\n","        t = torch.tensor([t])\n","        return (x.to(device), t.to(device))\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def params(self):\n","        return self.vocab_size, self.char_to_idx, self.idx_to_char"]},{"cell_type":"markdown","id":"H3iUTucewtRe","metadata":{"id":"H3iUTucewtRe"},"source":["This class is used to create a dataset of our Shakespeare corpus, which in turn is used to create a `DataLoader` instance."]},{"cell_type":"code","execution_count":null,"id":"dYFgoYnYtXWQ","metadata":{"id":"dYFgoYnYtXWQ"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","# Create data loader\n","train_data = CustomDataset('shakespeare')\n","vocab_size, char_to_idx, idx_to_char = train_data.params()\n","\n","seq_length = 100\n","train_loader = DataLoader(dataset=train_data,\n","                          batch_size=seq_length,\n","                          shuffle=False)"]},{"cell_type":"markdown","id":"94Ku7J3dyhOi","metadata":{"id":"94Ku7J3dyhOi","tags":[]},"source":["### Exercise\n","\n","What is the purpose of the `DataLoader` class? Check the PyTorch API in order to answer this question."]},{"cell_type":"markdown","id":"7ChyMoK3zhoG","metadata":{"id":"7ChyMoK3zhoG"},"source":["### Next"]},{"cell_type":"markdown","id":"sZwDoE81zpcq","metadata":{"id":"sZwDoE81zpcq"},"source":["The following cell defines a `CharLSTM` class, implementing a LSTM-based RNN."]},{"cell_type":"code","execution_count":null,"id":"fuXBtAB6tSrT","metadata":{"id":"fuXBtAB6tSrT"},"outputs":[],"source":["class CharLSTM(nn.Module):\n","    def __init__(self, vocab, hidden_size, n_layers=1):\n","        super(CharLSTM, self).__init__()\n","        self.n_layers = n_layers\n","        self.vocab_size = vocab\n","        self.hidden_size = hidden_size\n","        self.lstm = nn.LSTM(vocab, hidden_size, n_layers, batch_first=False)\n","        self.linear = nn.Linear(hidden_size, vocab, bias=True)\n","\n","    def forward(self, input, h0, c0):\n","        output, (hn, cn) = self.lstm(input, (h0, c0))\n","        scores = self.linear(output)\n","        return scores, hn, cn\n","\n","    def sample(self, x, txt_length=500):\n","        x = x.view(1, 1, self.vocab_size)\n","        h = torch.zeros(self.n_layers, 1, self.hidden_size).to(device)\n","        c = torch.zeros(self.n_layers, 1, self.hidden_size).to(device)\n","        txt = ''\n","        for i in range(txt_length):\n","            scores, h, c = self.forward(x, h, c)\n","            probs = nn.functional.softmax(scores, dim=2).view(self.vocab_size)\n","            pred = torch.tensor(list(WeightedRandomSampler(probs,\n","                                                           1,\n","                                                           replacement=True)))\n","            x = F.one_hot(pred, num_classes=self.vocab_size)\n","            x = (x.view(1, 1, self.vocab_size)\n","                  .type(torch.FloatTensor)\n","                  .to(device))\n","            next_character = idx_to_char[pred.item()]\n","            txt += next_character\n","        return txt"]},{"cell_type":"markdown","id":"UnBLyw522CgJ","metadata":{"id":"UnBLyw522CgJ"},"source":["The following cell creates an instance of `CharLSTM` and trains it for 1000 iterations, sampling the learnt distribution before starting to learn, as well as at half-time and at the end of the learning process."]},{"cell_type":"code","execution_count":null,"id":"Wdes7dlMgVNc","metadata":{"id":"Wdes7dlMgVNc"},"outputs":[],"source":["hidden_size = 250\n","n_layers = 1\n","lr = 0.01\n","\n","model = CharLSTM(vocab_size, hidden_size, n_layers=n_layers).to(device)\n","\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = Adam(model.parameters(), lr=lr)\n","\n","h = torch.zeros(n_layers, 1, hidden_size).to(device)\n","c = torch.zeros(n_layers, 1, hidden_size).to(device)\n","\n","i = 0\n","for inputs, targets in train_loader:\n","\n","    # Detach hidden and cell state to prevent backpropagation through them\n","    h, c = h.detach(), c.detach()\n","\n","    # Forward run the model and get predictions\n","    scores, h, c = model(inputs, h, c)\n","    loss = loss_fn(scores.squeeze(dim=1), targets.squeeze(dim=1))\n","\n","    # Backpropagate the loss and update parameters\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","    # Print loss and sample text every 500 steps\n","    if i % 500 == 0:\n","        print('-' * 80)\n","        print(f'{i}: {loss}')\n","        print(model.sample(inputs[0]))\n","        print('-' * 80)\n","    i += 1\n","    if i > 1000:\n","        break"]},{"cell_type":"markdown","id":"9f7a234e-53f7-4eb1-9642-dc727a92e44a","metadata":{"id":"9f7a234e-53f7-4eb1-9642-dc727a92e44a"},"source":["### Exercise"]},{"cell_type":"markdown","id":"722ef544-c9ff-4760-b3c8-78eae8f73a0b","metadata":{"id":"722ef544-c9ff-4760-b3c8-78eae8f73a0b"},"source":["Modify the previous cell so that the loss is saved each 50 iterations during learning. Run for 10000 iterations and graphically check that loss is being minimized. Sample from the learnt distribution, using different initial characters. Save the last sampled text in a variable named `generated_text`, as we will use it at the end of the practical part."]},{"cell_type":"markdown","id":"ff7cc9a7-0acd-4e37-82b8-e0745161706e","metadata":{"id":"ff7cc9a7-0acd-4e37-82b8-e0745161706e"},"source":["### Next"]},{"cell_type":"markdown","id":"60559ae4-550b-4284-90b1-94d704352080","metadata":{"id":"60559ae4-550b-4284-90b1-94d704352080"},"source":["The next cell loads a variant of the GPT-2 transformer."]},{"cell_type":"code","execution_count":null,"id":"77xuxQFrgbtR","metadata":{"id":"77xuxQFrgbtR","tags":[]},"outputs":[],"source":["model_id = 'openai-community/gpt2-large'\n","model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n","tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n","\n","test = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')[:50]\n","encodings = tokenizer('\\n\\n'.join(test['text']), return_tensors='pt')"]},{"cell_type":"markdown","id":"d41b3145-d0e5-4ef8-ba33-1bb8b5b4c561","metadata":{"id":"d41b3145-d0e5-4ef8-ba33-1bb8b5b4c561"},"source":["### Exercise"]},{"cell_type":"markdown","id":"cdf7f2b3-5e3b-4a6e-8cd6-71b0d45c545a","metadata":{"id":"cdf7f2b3-5e3b-4a6e-8cd6-71b0d45c545a"},"source":["What is the dimension of the GPT model which we are using (that is, the maximum length of a processable string)?"]},{"cell_type":"markdown","id":"85741efb-b855-46aa-bd39-66ac48925d00","metadata":{"id":"85741efb-b855-46aa-bd39-66ac48925d00"},"source":["### Exercise"]},{"cell_type":"markdown","id":"8fa8bec3-ea2d-4d73-8915-6beff2d0a6d2","metadata":{"id":"8fa8bec3-ea2d-4d73-8915-6beff2d0a6d2"},"source":["The code in the next cell computes the perplexity of a string generated by this model, where this string has a length higher than the model dimension. Run the code and take note of the obtained perplexity value. Is this procedure is in line with the technique?"]},{"cell_type":"code","execution_count":null,"id":"8bCPAnJWgl00","metadata":{"id":"8bCPAnJWgl00"},"outputs":[],"source":["max_length = model.config.n_positions\n","stride = 1024 # no overlap\n","seq_len = encodings.input_ids.size(1)\n","\n","nlls = []\n","prev_end_loc = 0\n","for begin_loc in tqdm(range(0, seq_len, stride)):\n","    end_loc = min(begin_loc + max_length, seq_len)\n","    trg_len = end_loc - prev_end_loc\n","    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n","    target_ids = input_ids.clone()\n","    target_ids[:, :-trg_len] = -100\n","\n","    with torch.no_grad():\n","        outputs = model(input_ids, labels=target_ids)\n","\n","        # loss is calculated using CrossEntropyLoss\n","        # which averages over valid labels\n","        # N.B. the model only calculates loss over trg_len - 1 labels, because\n","        # it internally shifts the labels to the left by 1.\n","        neg_log_likelihood = outputs.loss\n","\n","    nlls.append(neg_log_likelihood)\n","\n","    prev_end_loc = end_loc\n","    if end_loc == seq_len:\n","        break\n","\n","ppl = torch.exp(torch.stack(nlls).mean())\n","print(ppl)"]},{"cell_type":"markdown","id":"949a2002-2de5-43cc-8508-56d24a8f0425","metadata":{"id":"949a2002-2de5-43cc-8508-56d24a8f0425"},"source":["### Exercise"]},{"cell_type":"markdown","id":"7e4afe88-4c91-4010-a7ef-968cc7c19945","metadata":{"id":"7e4afe88-4c91-4010-a7ef-968cc7c19945"},"source":["Modify the previous cell so that perplexity is correctly estimated."]},{"cell_type":"markdown","id":"1e51e8bf-1946-4b39-b72c-196e413e5f66","metadata":{"id":"1e51e8bf-1946-4b39-b72c-196e413e5f66"},"source":["### Next"]},{"cell_type":"markdown","id":"5b1a2b70-6e40-4b66-a2a2-f287b9b5e0f5","metadata":{"tags":[],"id":"5b1a2b70-6e40-4b66-a2a2-f287b9b5e0f5"},"source":["We will not use anymore the GPT-2 model, thus we can free some RAM."]},{"cell_type":"code","execution_count":null,"id":"ABEL1hbXcd3v","metadata":{"id":"ABEL1hbXcd3v"},"outputs":[],"source":["def clear_memory():\n","  gc.collect()\n","  torch.cuda.empty_cache()\n","\n","del model\n","del tokenizer\n","clear_memory()"]},{"cell_type":"markdown","id":"02fffbc0-c865-4249-9790-776b53267718","metadata":{"id":"02fffbc0-c865-4249-9790-776b53267718"},"source":["The next cell loads the Emotion dataset which we have used during the lecture."]},{"cell_type":"code","execution_count":null,"id":"USe2EQzAv3HW","metadata":{"id":"USe2EQzAv3HW"},"outputs":[],"source":["emotions = load_dataset('emotion', trust_remote_code=True)"]},{"cell_type":"markdown","id":"5c111427-45bb-4bab-8f35-2050ed4ac600","metadata":{"id":"5c111427-45bb-4bab-8f35-2050ed4ac600"},"source":["### Exercise"]},{"cell_type":"markdown","id":"54891b7c-f3de-462a-91a3-de7333ab5204","metadata":{"id":"54891b7c-f3de-462a-91a3-de7333ab5204"},"source":["Select the first five examples in the training set, and show the corresponding tweet and encoded label. Peek into the training set object in order to figure out how to decode the numeric labels into the corresponding emotions."]},{"cell_type":"markdown","id":"d16347f6-cc7c-4718-b273-ab224cd86734","metadata":{"id":"d16347f6-cc7c-4718-b273-ab224cd86734"},"source":["### Next"]},{"cell_type":"markdown","id":"85cde951-2af8-4005-8e5b-621395a49e56","metadata":{"id":"85cde951-2af8-4005-8e5b-621395a49e56"},"source":["We will use from now on the DistilBERT model, whose tokenizer is downloaded from Hugging Face in the next cell."]},{"cell_type":"code","execution_count":null,"id":"P6D0CDMuyMoL","metadata":{"id":"P6D0CDMuyMoL"},"outputs":[],"source":["model_ckpt = 'distilbert-base-uncased'\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"]},{"cell_type":"markdown","id":"17ac6c13-252c-47c2-91bf-9557023dcfd0","metadata":{"id":"17ac6c13-252c-47c2-91bf-9557023dcfd0"},"source":["### Exercise"]},{"cell_type":"markdown","id":"cf320cfe-4ed7-451b-9dbb-e7a948037f37","metadata":{"id":"cf320cfe-4ed7-451b-9dbb-e7a948037f37"},"source":["Encode the 10-th tweet in the training set using the tokenizer associated to DistilBERT. Decode the obtained embeddings and check if some special tokens have been added."]},{"cell_type":"markdown","id":"a079be11-c7ae-4080-9c60-5fdbd85eda31","metadata":{"id":"a079be11-c7ae-4080-9c60-5fdbd85eda31"},"source":["### Next"]},{"cell_type":"markdown","id":"d59c6eed-6400-44ff-b994-7b3aacd06020","metadata":{"id":"d59c6eed-6400-44ff-b994-7b3aacd06020"},"source":["The following cell converts temporarily the display format of the training set, showing it as a Pandas DataFrame."]},{"cell_type":"code","execution_count":null,"id":"ksqIeno5SITZ","metadata":{"id":"ksqIeno5SITZ"},"outputs":[],"source":["emotions.set_format(type='pandas')\n","df = emotions['train'][:]\n","df.head()"]},{"cell_type":"markdown","id":"046ec3bf-6251-4983-a7ce-b85d5e69c406","metadata":{"id":"046ec3bf-6251-4983-a7ce-b85d5e69c406"},"source":["### Exercise"]},{"cell_type":"markdown","id":"sn62gMhiSrEr","metadata":{"id":"sn62gMhiSrEr"},"source":["Add to the previous DataFrame a `label_name` column containing the emotion names for each tween."]},{"cell_type":"markdown","id":"2ff97f69-1aef-4e54-84d8-2b41957b3b28","metadata":{"id":"2ff97f69-1aef-4e54-84d8-2b41957b3b28"},"source":["### Exercise"]},{"cell_type":"markdown","id":"f55c06b3-4b02-41fc-bb63-0753978b5547","metadata":{"id":"f55c06b3-4b02-41fc-bb63-0753978b5547"},"source":["Show a bar plot for the absolute frequencies of each emotion in the train set."]},{"cell_type":"markdown","id":"01a11d46-209e-4ecf-9cca-b922cc751b6f","metadata":{"id":"01a11d46-209e-4ecf-9cca-b922cc751b6f"},"source":["### Exercise"]},{"cell_type":"markdown","id":"7089854b-4c29-4cf0-b828-4e86838ab326","metadata":{"id":"7089854b-4c29-4cf0-b828-4e86838ab326"},"source":["For each different emotion, show a box plot illustrating the corresponding distribution of tweet lengths (in terms of their number of words). If possible, align all these plots in a same graphic."]},{"cell_type":"markdown","id":"f6f495f9-2856-408b-b561-661ec86bbf23","metadata":{"id":"f6f495f9-2856-408b-b561-661ec86bbf23"},"source":["### Next"]},{"cell_type":"markdown","id":"2735df76-8513-450e-8c41-b690eabd73eb","metadata":{"id":"2735df76-8513-450e-8c41-b690eabd73eb"},"source":["The next cell resets the display form of the train set to its default, and defines a `tokenize` function which is then used to add to the train set also the tokenized versions of all tweets."]},{"cell_type":"code","execution_count":null,"id":"tYr_BoS4U7Kp","metadata":{"id":"tYr_BoS4U7Kp"},"outputs":[],"source":["emotions.reset_format()\n","\n","def tokenize(batch):\n","    return tokenizer(batch['text'], padding=True, truncation=True)\n","\n","emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)"]},{"cell_type":"markdown","id":"eb825660-b003-4975-80da-556ade1941d8","metadata":{"id":"eb825660-b003-4975-80da-556ade1941d8"},"source":["### Exercise"]},{"cell_type":"markdown","id":"ccf940b7-b8a6-4785-af2f-610ecb173ec7","metadata":{"id":"ccf940b7-b8a6-4785-af2f-610ecb173ec7"},"source":["Check that the train dataset has been augmented with the extracted encodings."]},{"cell_type":"markdown","id":"edc91fb0-9659-4362-a712-6ccb0cc183af","metadata":{"id":"edc91fb0-9659-4362-a712-6ccb0cc183af"},"source":["### Next"]},{"cell_type":"markdown","id":"72ae2ea9-2f6e-400f-8cc3-97ad547978ed","metadata":{"id":"72ae2ea9-2f6e-400f-8cc3-97ad547978ed"},"source":["The next cell loads the DistilBERT model and shows how to take an input, feed it to the model, and extract the states of the last hidden layer (that is, the learnt latent encoding for all tokens in the input)."]},{"cell_type":"code","execution_count":null,"id":"OTf_MzdkVIMG","metadata":{"id":"OTf_MzdkVIMG"},"outputs":[],"source":["model_ckpt = 'distilbert-base-uncased'\n","model = AutoModel.from_pretrained(model_ckpt).to(device)"]},{"cell_type":"code","execution_count":null,"id":"xDlnXF5eVaKG","metadata":{"id":"xDlnXF5eVaKG"},"outputs":[],"source":["text = 'this is a test'\n","inputs = tokenizer(text, return_tensors='pt')\n","print(f'Input tensor shape: {inputs['input_ids'].size()}')\n","\n","inputs = {k:v.to(device) for k,v in inputs.items()}\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","print(outputs)\n","\n","print(f'Last hidden state tensor shape: {outputs.last_hidden_state.size()}')"]},{"cell_type":"markdown","id":"35250d7e-db2a-44e5-95f7-279ab785b39d","metadata":{"id":"35250d7e-db2a-44e5-95f7-279ab785b39d"},"source":["### Exercise"]},{"cell_type":"markdown","id":"c750108a-a71d-4f5b-ad14-043d10e6a682","metadata":{"id":"c750108a-a71d-4f5b-ad14-043d10e6a682"},"source":["The following function accepts a batch of inputs and returns a dictionary associating the key `hidden_state` to the latent encoding of the first token of each input (that is, the `[CLS]` token). Fill in the missing parts, being sure to place the obtained tensors in the CPU and to subsequently convert them to numpy arrays."]},{"cell_type":"code","execution_count":null,"id":"56af3108-08c2-4d0c-bb17-cf5c79ce3764","metadata":{"id":"56af3108-08c2-4d0c-bb17-cf5c79ce3764"},"outputs":[],"source":["def extract_hidden_states(batch):\n","    # Place model inputs on the GPU\n","    inputs = <FILL ME>\n","    # Extract last hidden states\n","    with torch.no_grad():\n","        <FILL ME>\n","    # Return vector for [CLS] token\n","    return {'hidden_state': <FILL ME>}"]},{"cell_type":"markdown","id":"9ce605fc-778e-4a8f-9e8f-a5505be16a7d","metadata":{"id":"9ce605fc-778e-4a8f-9e8f-a5505be16a7d"},"source":["### Next"]},{"cell_type":"markdown","id":"8773aaee-1713-42c4-bc19-d078b3f4e95a","metadata":{"id":"8773aaee-1713-42c4-bc19-d078b3f4e95a"},"source":["The next cell applies the `extract_hidden_states` to the train set augmented with the tweet encondings. Note that it is necessary to convert to the internal PyTorch format the columns which have been added."]},{"cell_type":"code","execution_count":null,"id":"f1_Eaw3nV-GN","metadata":{"id":"f1_Eaw3nV-GN"},"outputs":[],"source":["emotions_encoded.set_format('torch',\n","                            columns=['input_ids', 'attention_mask', 'label'])\n","emotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True)"]},{"cell_type":"markdown","id":"7afe03d4-c12b-4173-9a3c-f0ac2947b0ab","metadata":{"id":"7afe03d4-c12b-4173-9a3c-f0ac2947b0ab"},"source":["### Exercise"]},{"cell_type":"markdown","id":"02ae99af-67fd-4660-a8c7-787ce95565b7","metadata":{"id":"02ae99af-67fd-4660-a8c7-787ce95565b7"},"source":["Visualize the column names of the dataset obtained after `extract_hidden_states` has been applied, and check that a new column with the extracted latent encodings have been added."]},{"cell_type":"markdown","id":"a04fdac1-74cb-4793-b239-d872012dff2d","metadata":{"id":"a04fdac1-74cb-4793-b239-d872012dff2d"},"source":["### Next"]},{"cell_type":"markdown","id":"cd79eb50-8456-40a9-a582-683519d378b6","metadata":{"id":"cd79eb50-8456-40a9-a582-683519d378b6"},"source":["The following cell applies the UMAP dimensionality reduction to obtain a 2D compressed representation of the latent encodings."]},{"cell_type":"code","execution_count":null,"id":"rqYu8nPBWPl-","metadata":{"id":"rqYu8nPBWPl-"},"outputs":[],"source":["# Scale features to [0,1] range\n","X_scaled = MinMaxScaler().fit_transform(X_train)\n","# Initialize and fit UMAP\n","mapper = UMAP(n_components=2, metric='cosine').fit(X_scaled)\n","# Create a DataFrame of 2D embeddings\n","df_emb = pd.DataFrame(mapper.embedding_, columns=['X', 'Y'])\n","df_emb['label'] = y_train\n","df_emb.head()"]},{"cell_type":"markdown","id":"40a77a6b-3dbe-4b61-8606-596d13b9a4e8","metadata":{"id":"40a77a6b-3dbe-4b61-8606-596d13b9a4e8"},"source":["### Exercise"]},{"cell_type":"markdown","id":"b44c7e12-da96-4ba9-8d56-3be502d80b07","metadata":{"id":"b44c7e12-da96-4ba9-8d56-3be502d80b07"},"source":["Considering each emotion separately, show a graphic (for instance, an hexbin graph--check the matplotlib documentation for the details) that depicts how are distributed all the 2D points in `df_emb` which correspond to that emotion."]},{"cell_type":"markdown","id":"52b1f238-ab73-41c6-b37d-6fb1111aa5ec","metadata":{"id":"52b1f238-ab73-41c6-b37d-6fb1111aa5ec"},"source":["### Exercise"]},{"cell_type":"markdown","id":"cc3c656c-90ae-48a2-b5d7-9636073ed50a","metadata":{"id":"cc3c656c-90ae-48a2-b5d7-9636073ed50a"},"source":["Use the data in `X_train` and `y_train` to train a logistic regression model, setting the maximum number of iterations to 3000. Compute the accuracy of the learnt model on the validation set that you extracted before and visualize the corresponding confusion matrix."]},{"cell_type":"markdown","id":"e386e539-0ad3-423d-9652-c72e61abada4","metadata":{"id":"e386e539-0ad3-423d-9652-c72e61abada4"},"source":["### Exercise"]},{"cell_type":"markdown","id":"876dee36-9231-4971-8833-9ef3a4f0fc72","metadata":{"id":"876dee36-9231-4971-8833-9ef3a4f0fc72"},"source":["Repeat the previous exercise using another supervised ML model of your choice."]},{"cell_type":"markdown","id":"e7304835-75ba-43f5-a710-13ea4f521d4f","metadata":{"id":"e7304835-75ba-43f5-a710-13ea4f521d4f"},"source":["### Exercise"]},{"cell_type":"markdown","id":"0403ed75-eeae-4e80-bca9-0c0647d74c37","metadata":{"id":"0403ed75-eeae-4e80-bca9-0c0647d74c37"},"source":["Attach a classification head to the previously loaded DistilBERT model, using the `AutoModelForSequenceClassification` class."]},{"cell_type":"markdown","id":"7a9dd6b3-f5dc-4020-aae3-9aca33fdc342","metadata":{"id":"7a9dd6b3-f5dc-4020-aae3-9aca33fdc342"},"source":["#### Exercise"]},{"cell_type":"markdown","id":"bc964f9f-912c-4159-90f8-e0c80642980a","metadata":{"id":"bc964f9f-912c-4159-90f8-e0c80642980a"},"source":["While running the code for the previous exercise, you should have obtained a warning. Is this correct?"]},{"cell_type":"markdown","id":"57d40128-2cd4-4519-af26-e677960146d7","metadata":{"id":"57d40128-2cd4-4519-af26-e677960146d7"},"source":["### Next"]},{"cell_type":"markdown","id":"eb4f40f5-6d92-4bf6-84de-1e82a2789db6","metadata":{"id":"eb4f40f5-6d92-4bf6-84de-1e82a2789db6"},"source":["The following cells define the objects which are needed in order to train the model created in the previous exercise."]},{"cell_type":"code","execution_count":null,"id":"4rQU-uOEbqg2","metadata":{"id":"4rQU-uOEbqg2"},"outputs":[],"source":["batch_size = 64\n","logging_steps = len(emotions_encoded['train']) // batch_size\n","model_name = f'{model_ckpt}-finetuned-emotion'\n","training_args = TrainingArguments(output_dir=model_name,\n","                                  num_train_epochs=2,\n","                                  learning_rate=2e-5,\n","                                  per_device_train_batch_size=batch_size,\n","                                  per_device_eval_batch_size=batch_size,\n","                                  weight_decay=0.01,\n","                                  evaluation_strategy='epoch',\n","                                  disable_tqdm=False,\n","                                  logging_steps=logging_steps,\n","                                  log_level='error')"]},{"cell_type":"code","execution_count":null,"id":"Z1ol9Q4_g7dc","metadata":{"id":"Z1ol9Q4_g7dc"},"outputs":[],"source":["def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    f1 = f1_score(labels, preds, average='weighted')\n","    acc = accuracy_score(labels, preds)\n","    return {'accuracy': acc, 'f1': f1}"]},{"cell_type":"markdown","id":"517adf7b-68b9-4a6d-b792-8fb0546d2dd5","metadata":{"id":"517adf7b-68b9-4a6d-b792-8fb0546d2dd5"},"source":["### Exercise"]},{"cell_type":"markdown","id":"9ab566a5-f5e7-4314-b186-f09988a160db","metadata":{"id":"9ab566a5-f5e7-4314-b186-f09988a160db"},"source":["Use the `Trainer` class in order to train the model we have created (hint: have a look at the official documentation, and if something is not correct you can see the answer in the slides of the lecture)."]},{"cell_type":"markdown","id":"1a85abb6-2feb-48a2-85e3-ebfe6e6eecfc","metadata":{"id":"1a85abb6-2feb-48a2-85e3-ebfe6e6eecfc"},"source":["### Exercise"]},{"cell_type":"markdown","id":"b4c1346b-8b92-4d7f-9ba3-cbc047e3381a","metadata":{"id":"b4c1346b-8b92-4d7f-9ba3-cbc047e3381a"},"source":["Compute the performance metrics for the trained model, and show the corresponding confusion matrix, comparing it with the one of the model based on logistic regression."]},{"cell_type":"markdown","id":"9e5279d2-b515-4020-aef2-93a7ba47ab94","metadata":{"id":"9e5279d2-b515-4020-aef2-93a7ba47ab94"},"source":["### Exercise"]},{"cell_type":"markdown","id":"2d9e3c81-17e4-4b14-9478-ac095dc16419","metadata":{"id":"2d9e3c81-17e4-4b14-9478-ac095dc16419"},"source":["Play with the data provided to `TrainingArguments` in order to slighlty modify the learning process."]},{"cell_type":"markdown","id":"e5eda700-7f34-4027-9857-f62544b7d98c","metadata":{"id":"e5eda700-7f34-4027-9857-f62544b7d98c"},"source":["### Exercise"]},{"cell_type":"markdown","id":"4c7ab9aa-9f5a-4856-ba3c-9c5a18d79c98","metadata":{"id":"4c7ab9aa-9f5a-4856-ba3c-9c5a18d79c98"},"source":["Complete the following cell so that the BLEU score for the provided texts in `source` and `summary` is computed."]},{"cell_type":"code","execution_count":null,"id":"f9b52287-9f62-4ff9-9c92-cdc5ef193357","metadata":{"id":"f9b52287-9f62-4ff9-9c92-cdc5ef193357"},"outputs":[],"source":["from datasets import load_metric\n","\n","source = 'It is reported that on April, 23rd a lunar eclipse will be visible'\n","         ' between 10PM and 11PM in the surroundings of Athens.'\n","summary = 'A lunar eclipse will be visible around Athens during the night'\n","          ' of April, 23rd.'\n","\n","<FILL ME>"]},{"cell_type":"markdown","id":"d8932411-4dfc-44e8-88c6-270c06899f1b","metadata":{"id":"d8932411-4dfc-44e8-88c6-270c06899f1b"},"source":["#### Exercise"]},{"cell_type":"markdown","id":"aa47572c-029c-4a28-a3f5-0966e4dff6b5","metadata":{"id":"aa47572c-029c-4a28-a3f5-0966e4dff6b5"},"source":["Repeat the previous exercise, now using `'Athens Athens Athens Athens Athens Athens'` as summary."]},{"cell_type":"markdown","id":"be5e1a05-1003-4eac-bc9f-77855d805094","metadata":{"id":"be5e1a05-1003-4eac-bc9f-77855d805094"},"source":["### Exercise"]},{"cell_type":"markdown","id":"bb1fb8f0-af63-42eb-9c2c-736477142afa","metadata":{"id":"bb1fb8f0-af63-42eb-9c2c-736477142afa"},"source":["Compute the ROUGE score for the first summary. Note that the output is richer than with BLUE: for each of the variants of the ROUGE score, only select the `f1measure` value of the `mid` field."]},{"cell_type":"markdown","id":"bd99933f-a6d0-4467-bfe2-4523e0876037","metadata":{"id":"bd99933f-a6d0-4467-bfe2-4523e0876037"},"source":["### Exercise"]},{"cell_type":"markdown","id":"65297bf6-83dc-4441-9f39-7cdc2e285386","metadata":{"id":"65297bf6-83dc-4441-9f39-7cdc2e285386"},"source":["Compute the BLUE score of the text generated in the last exercise using RNNs (you should have saved it in a `generated_text` variable). Note that the source is in this case the whole corpus, which is available in the `data` field of the `CustomDataset` object which you created."]},{"cell_type":"markdown","id":"f4ab62c8-4f0e-47cf-aae8-36a0e731269e","metadata":{"id":"f4ab62c8-4f0e-47cf-aae8-36a0e731269e"},"source":["### Exercise"]},{"cell_type":"markdown","id":"7f8a9e74-ea4f-4daf-b74c-6f588c8cc883","metadata":{"id":"7f8a9e74-ea4f-4daf-b74c-6f588c8cc883"},"source":["Download from Hugging Face any model for summarization and use it on a text of your choice (you can use, for instance, the example text provided in the card of the model you have downloaded). Compute the BLUE and ROUGE scores for the obtained summarization."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"required_libs":[]},"nbformat":4,"nbformat_minor":5}